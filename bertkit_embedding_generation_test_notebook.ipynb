{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "275154e9",
   "metadata": {},
   "source": [
    "### 1. Load the HuggingFace BERT reference model and its tokenizer for the tests, variant: bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d198a86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23f3b9e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters (original): 109482240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "hf_tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "hf_bert_model = BertModel.from_pretrained(model_name) # weights are now loaded!\n",
    "modelparams = sum(p.numel() for p in hf_bert_model.parameters())\n",
    "print(f\"Number of model parameters (original): {modelparams}\")\n",
    "# should yield: 109482240\n",
    "hf_bert_model.eval()   # set model to evaluation mode (dropouts deactivated for inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0347085d",
   "metadata": {},
   "source": [
    "### 2. Tokenization of the text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58d09f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokendata:\n",
      "==========\n",
      "\n",
      "input_ids : tensor([[  101,  7592,  1010,  2023,  2003,  1037,  7099,  6251,  2005, 14324,\n",
      "          7861,  8270,  4667,  4245,  1012,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  1037,  2460,  3231,  6251,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2800, 10962,  1024, 26191,  1010,  7222, 23804,  2015,  1010,\n",
      "          4589,  2015,  1010, 18108,  1010, 11463,  5644,  1010, 24188,  5134,\n",
      "          1010, 13137, 20968,   102]])\n",
      "token_type_ids : tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask : tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Hello, this is a sample sentence for BERT embedding generation.\",\n",
    "    \"A short test sentence\",\n",
    "    \"Available fruits: bananas, pineapples, oranges, apples, melons, cherries, strawberries\"\n",
    "]\n",
    "\n",
    "tokendata = hf_tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "print(\"tokendata:\\n==========\\n\")\n",
    "for k, v in tokendata.items():\n",
    "    print(k,\":\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f03f183",
   "metadata": {},
   "source": [
    "### 3. Text inference and embedding generation,  transformer output structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e882a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['last_hidden_state', 'pooler_output'])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = hf_bert_model(**tokendata)\n",
    "print(dict(outputs).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83ad20de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2303, -0.4416, -0.2611,  ..., -0.7063, -0.0347,  0.6870],\n",
       "        [-0.1313, -0.1393, -0.3229,  ..., -0.0999,  0.0072,  0.2108],\n",
       "        [-0.2609, -0.1264, -0.3669,  ..., -0.1132,  0.1674,  0.1579]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings are the representation of the CLS token in each batch (sequence)\n",
    "# This representation is trained during pretraining:\n",
    "outputs.last_hidden_state[:, 0, :]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b2f7800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8546, -0.5405, -0.8914,  ..., -0.8892, -0.6927,  0.8527],\n",
       "        [-0.6780, -0.1119,  0.6431,  ...,  0.3919, -0.5026,  0.6840],\n",
       "        [-0.8838, -0.5582, -0.9850,  ..., -0.9025, -0.7519,  0.8843]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.pooler_output # not needed here. main purpose for next sentence prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732d71aa",
   "metadata": {},
   "source": [
    "# Now instantiating the BERTKit model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df732c8",
   "metadata": {},
   "source": [
    "### 4. Loading the BERTKit configuration, model, tokenizer and weight loader\n",
    "In contrast to the HF Bert model, there is no .from_pretrained method that instantiates the model together with its correct weights. Instead, we instantiate the layer structure in exactly the same way where the weights are uninitialized. Then we expicitly load the weights with a little tool from the model.bin file provided by HF. It is importatnt that the naming of the layers and the function hierarchy remains unaltered for the moment because the naming of stored layers exactly follows that scheme. Another degree of freedom that we have in our version is the explicit injection of the overall model configuration. Everything is implicitly done in HF during the loading phase but in our procedure we have much more control albeit we strictly adhere to the architecture to be able to ingest the pretrained weigths. When the BERT model is loaded from HF and the configuration printed we obtain:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "357f45cb",
   "metadata": {},
   "source": [
    "# original from HF:\n",
    "BertConfig {\n",
    "  \"_name_or_path\": \"bert-base-uncased\",\n",
    "  \"architectures\": [\n",
    "    \"BertForMaskedLM\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"classifier_dropout\": null,\n",
    "  \"gradient_checkpointing\": false,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-12,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"model_type\": \"bert\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"position_embedding_type\": \"absolute\",\n",
    "  \"transformers_version\": \"4.36.2\",\n",
    "  \"type_vocab_size\": 2,\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 30522\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04b7e704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertkit_modules.bert_config import BertConfig\n",
    "from bertkit_modules.bert_model import BertModel\n",
    "from bertkit_modules.weightloader import WeightLoader\n",
    "from bertkit_tokenizer.bertkit_tokenizer import BertKitTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b880d05",
   "metadata": {},
   "source": [
    "Due to elimination of some features a few variables in the config class are not needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "137f0cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = {\n",
    "  \"attention_probs_dropout_prob\": 0.1, #\n",
    "  \"chunk_size_feed_forward\": 0, #\n",
    "  \"classifier_dropout\": None, #\n",
    "  \"hidden_act\": \"gelu\", # \n",
    "  \"hidden_dropout_prob\": 0.1, #\n",
    "  \"hidden_size\": 768, # \n",
    "  \"initializer_range\": 0.02, #\n",
    "  \"intermediate_size\": 3072, # \n",
    "  \"layer_norm_eps\": 1e-12, #\n",
    "  \"max_position_embeddings\": 512, #\n",
    "  \"num_attention_heads\": 12, # \n",
    "  \"num_hidden_layers\": 12, # \n",
    "  \"pad_token_id\": 0, #\n",
    "  \"type_vocab_size\": 2, #\n",
    "  \"output_attentions\": True, #\n",
    "  \"output_hidden_states\": True, #\n",
    "  \"vocab_size\": 30522 # \n",
    "}\n",
    "config = BertConfig(**bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ec7834a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> len vocab: 30522\n",
      "Number of model parameters (BERTKit): 109482240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bk_bert_model = BertModel(config)   # bk = BERTKit\n",
    "bk_tokenizer = BertKitTokenizer(\"./bertkit_tokenizer/vocab.txt\")\n",
    "modelparams = sum(p.numel() for p in bk_bert_model.parameters())\n",
    "print(f\"Number of model parameters (BERTKit): {modelparams}\")\n",
    "# should also yield: 109482240\n",
    "bk_bert_model.eval()   # set model to evaluation mode (dropouts deactivated for inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c5d2d9",
   "metadata": {},
   "source": [
    "### 5. Loading pretrained BERT weights from LFS repo into model. By using a weight loader we have full control over namings. Some layer name adjustments had to be done in order to load weights correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62cb4843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from: /home/markus/data/bert_base_uncased/pytorch_model.bin\n",
      "Found 207 parameters in weight file\n",
      "  bert.embeddings.word_embeddings.weight: torch.Size([30522, 768]) (torch.float32)\n",
      "  bert.embeddings.position_embeddings.weight: torch.Size([512, 768]) (torch.float32)\n",
      "  bert.embeddings.token_type_embeddings.weight: torch.Size([2, 768]) (torch.float32)\n",
      "  bert.embeddings.LayerNorm.gamma: torch.Size([768]) (torch.float32)\n",
      "  bert.embeddings.LayerNorm.beta: torch.Size([768]) (torch.float32)\n",
      "  bert.encoder.layer.0.attention.self.query.weight: torch.Size([768, 768]) (torch.float32)\n",
      "  ... and 201 more parameters\n",
      "\n",
      "Processing layer: embeddings.word_embeddings\n",
      "  ✓ embeddings.word_embeddings.weight: torch.Size([30522, 768])\n",
      "  Layer summary: 1/1 parameters loaded\n",
      "\n",
      "Processing layer: embeddings.position_embeddings\n",
      "  ✓ embeddings.position_embeddings.weight: torch.Size([512, 768])\n",
      "  Layer summary: 1/1 parameters loaded\n",
      "\n",
      "Processing layer: embeddings.token_type_embeddings\n",
      "  ✓ embeddings.token_type_embeddings.weight: torch.Size([2, 768])\n",
      "  Layer summary: 1/1 parameters loaded\n",
      "\n",
      "Processing layer: embeddings.LayerNorm\n",
      "  ✓ embeddings.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ embeddings.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.0.attention.self.query\n",
      "  ✓ encoder.layer.0.attention.self.query.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.0.attention.self.query.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.0.attention.self.key\n",
      "  ✓ encoder.layer.0.attention.self.key.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.0.attention.self.key.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.0.attention.self.value\n",
      "  ✓ encoder.layer.0.attention.self.value.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.0.attention.self.value.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.0.attention.output.dense\n",
      "  ✓ encoder.layer.0.attention.output.dense.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.0.attention.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.0.attention.output.LayerNorm\n",
      "  ✓ encoder.layer.0.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.0.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.0.intermediate.dense\n",
      "  ✓ encoder.layer.0.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "  ✓ encoder.layer.0.intermediate.dense.bias: torch.Size([3072])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.0.output.dense\n",
      "  ✓ encoder.layer.0.output.dense.weight: torch.Size([768, 3072])\n",
      "  ✓ encoder.layer.0.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.0.output.LayerNorm\n",
      "  ✓ encoder.layer.0.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.0.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.1.attention.self.query\n",
      "  ✓ encoder.layer.1.attention.self.query.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.1.attention.self.query.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.1.attention.self.key\n",
      "  ✓ encoder.layer.1.attention.self.key.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.1.attention.self.key.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.1.attention.self.value\n",
      "  ✓ encoder.layer.1.attention.self.value.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.1.attention.self.value.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.1.attention.output.dense\n",
      "  ✓ encoder.layer.1.attention.output.dense.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.1.attention.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.1.attention.output.LayerNorm\n",
      "  ✓ encoder.layer.1.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.1.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.1.intermediate.dense\n",
      "  ✓ encoder.layer.1.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "  ✓ encoder.layer.1.intermediate.dense.bias: torch.Size([3072])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.1.output.dense\n",
      "  ✓ encoder.layer.1.output.dense.weight: torch.Size([768, 3072])\n",
      "  ✓ encoder.layer.1.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.1.output.LayerNorm\n",
      "  ✓ encoder.layer.1.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.1.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.2.attention.self.query\n",
      "  ✓ encoder.layer.2.attention.self.query.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.2.attention.self.query.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.2.attention.self.key\n",
      "  ✓ encoder.layer.2.attention.self.key.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.2.attention.self.key.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.2.attention.self.value\n",
      "  ✓ encoder.layer.2.attention.self.value.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.2.attention.self.value.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.2.attention.output.dense\n",
      "  ✓ encoder.layer.2.attention.output.dense.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.2.attention.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.2.attention.output.LayerNorm\n",
      "  ✓ encoder.layer.2.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.2.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.2.intermediate.dense\n",
      "  ✓ encoder.layer.2.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "  ✓ encoder.layer.2.intermediate.dense.bias: torch.Size([3072])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.2.output.dense\n",
      "  ✓ encoder.layer.2.output.dense.weight: torch.Size([768, 3072])\n",
      "  ✓ encoder.layer.2.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.2.output.LayerNorm\n",
      "  ✓ encoder.layer.2.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.2.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.3.attention.self.query\n",
      "  ✓ encoder.layer.3.attention.self.query.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.3.attention.self.query.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.3.attention.self.key\n",
      "  ✓ encoder.layer.3.attention.self.key.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.3.attention.self.key.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.3.attention.self.value\n",
      "  ✓ encoder.layer.3.attention.self.value.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.3.attention.self.value.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.3.attention.output.dense\n",
      "  ✓ encoder.layer.3.attention.output.dense.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.3.attention.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.3.attention.output.LayerNorm\n",
      "  ✓ encoder.layer.3.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.3.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.3.intermediate.dense\n",
      "  ✓ encoder.layer.3.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "  ✓ encoder.layer.3.intermediate.dense.bias: torch.Size([3072])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.3.output.dense\n",
      "  ✓ encoder.layer.3.output.dense.weight: torch.Size([768, 3072])\n",
      "  ✓ encoder.layer.3.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.3.output.LayerNorm\n",
      "  ✓ encoder.layer.3.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.3.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.4.attention.self.query\n",
      "  ✓ encoder.layer.4.attention.self.query.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.4.attention.self.query.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.4.attention.self.key\n",
      "  ✓ encoder.layer.4.attention.self.key.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.4.attention.self.key.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.4.attention.self.value\n",
      "  ✓ encoder.layer.4.attention.self.value.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.4.attention.self.value.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.4.attention.output.dense\n",
      "  ✓ encoder.layer.4.attention.output.dense.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.4.attention.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.4.attention.output.LayerNorm\n",
      "  ✓ encoder.layer.4.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.4.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.4.intermediate.dense\n",
      "  ✓ encoder.layer.4.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "  ✓ encoder.layer.4.intermediate.dense.bias: torch.Size([3072])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.4.output.dense\n",
      "  ✓ encoder.layer.4.output.dense.weight: torch.Size([768, 3072])\n",
      "  ✓ encoder.layer.4.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.4.output.LayerNorm\n",
      "  ✓ encoder.layer.4.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.4.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.5.attention.self.query\n",
      "  ✓ encoder.layer.5.attention.self.query.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.5.attention.self.query.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.5.attention.self.key\n",
      "  ✓ encoder.layer.5.attention.self.key.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.5.attention.self.key.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.5.attention.self.value\n",
      "  ✓ encoder.layer.5.attention.self.value.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.5.attention.self.value.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.5.attention.output.dense\n",
      "  ✓ encoder.layer.5.attention.output.dense.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.5.attention.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.5.attention.output.LayerNorm\n",
      "  ✓ encoder.layer.5.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.5.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.5.intermediate.dense\n",
      "  ✓ encoder.layer.5.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "  ✓ encoder.layer.5.intermediate.dense.bias: torch.Size([3072])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.5.output.dense\n",
      "  ✓ encoder.layer.5.output.dense.weight: torch.Size([768, 3072])\n",
      "  ✓ encoder.layer.5.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.5.output.LayerNorm\n",
      "  ✓ encoder.layer.5.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.5.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.6.attention.self.query\n",
      "  ✓ encoder.layer.6.attention.self.query.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.6.attention.self.query.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.6.attention.self.key\n",
      "  ✓ encoder.layer.6.attention.self.key.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.6.attention.self.key.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.6.attention.self.value\n",
      "  ✓ encoder.layer.6.attention.self.value.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.6.attention.self.value.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.6.attention.output.dense\n",
      "  ✓ encoder.layer.6.attention.output.dense.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.6.attention.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.6.attention.output.LayerNorm\n",
      "  ✓ encoder.layer.6.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.6.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.6.intermediate.dense\n",
      "  ✓ encoder.layer.6.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "  ✓ encoder.layer.6.intermediate.dense.bias: torch.Size([3072])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.6.output.dense\n",
      "  ✓ encoder.layer.6.output.dense.weight: torch.Size([768, 3072])\n",
      "  ✓ encoder.layer.6.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.6.output.LayerNorm\n",
      "  ✓ encoder.layer.6.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.6.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.7.attention.self.query\n",
      "  ✓ encoder.layer.7.attention.self.query.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.7.attention.self.query.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.7.attention.self.key\n",
      "  ✓ encoder.layer.7.attention.self.key.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.7.attention.self.key.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.7.attention.self.value\n",
      "  ✓ encoder.layer.7.attention.self.value.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.7.attention.self.value.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.7.attention.output.dense\n",
      "  ✓ encoder.layer.7.attention.output.dense.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.7.attention.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.7.attention.output.LayerNorm\n",
      "  ✓ encoder.layer.7.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.7.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.7.intermediate.dense\n",
      "  ✓ encoder.layer.7.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "  ✓ encoder.layer.7.intermediate.dense.bias: torch.Size([3072])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.7.output.dense\n",
      "  ✓ encoder.layer.7.output.dense.weight: torch.Size([768, 3072])\n",
      "  ✓ encoder.layer.7.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.7.output.LayerNorm\n",
      "  ✓ encoder.layer.7.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.7.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.8.attention.self.query\n",
      "  ✓ encoder.layer.8.attention.self.query.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.8.attention.self.query.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.8.attention.self.key\n",
      "  ✓ encoder.layer.8.attention.self.key.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.8.attention.self.key.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.8.attention.self.value\n",
      "  ✓ encoder.layer.8.attention.self.value.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.8.attention.self.value.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.8.attention.output.dense\n",
      "  ✓ encoder.layer.8.attention.output.dense.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.8.attention.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.8.attention.output.LayerNorm\n",
      "  ✓ encoder.layer.8.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.8.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.8.intermediate.dense\n",
      "  ✓ encoder.layer.8.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "  ✓ encoder.layer.8.intermediate.dense.bias: torch.Size([3072])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.8.output.dense\n",
      "  ✓ encoder.layer.8.output.dense.weight: torch.Size([768, 3072])\n",
      "  ✓ encoder.layer.8.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.8.output.LayerNorm\n",
      "  ✓ encoder.layer.8.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.8.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.9.attention.self.query\n",
      "  ✓ encoder.layer.9.attention.self.query.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.9.attention.self.query.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.9.attention.self.key\n",
      "  ✓ encoder.layer.9.attention.self.key.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.9.attention.self.key.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.9.attention.self.value\n",
      "  ✓ encoder.layer.9.attention.self.value.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.9.attention.self.value.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.9.attention.output.dense\n",
      "  ✓ encoder.layer.9.attention.output.dense.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.9.attention.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.9.attention.output.LayerNorm\n",
      "  ✓ encoder.layer.9.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.9.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.9.intermediate.dense\n",
      "  ✓ encoder.layer.9.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "  ✓ encoder.layer.9.intermediate.dense.bias: torch.Size([3072])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.9.output.dense\n",
      "  ✓ encoder.layer.9.output.dense.weight: torch.Size([768, 3072])\n",
      "  ✓ encoder.layer.9.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.9.output.LayerNorm\n",
      "  ✓ encoder.layer.9.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.9.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.10.attention.self.query\n",
      "  ✓ encoder.layer.10.attention.self.query.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.10.attention.self.query.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.10.attention.self.key\n",
      "  ✓ encoder.layer.10.attention.self.key.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.10.attention.self.key.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.10.attention.self.value\n",
      "  ✓ encoder.layer.10.attention.self.value.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.10.attention.self.value.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.10.attention.output.dense\n",
      "  ✓ encoder.layer.10.attention.output.dense.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.10.attention.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.10.attention.output.LayerNorm\n",
      "  ✓ encoder.layer.10.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.10.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.10.intermediate.dense\n",
      "  ✓ encoder.layer.10.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "  ✓ encoder.layer.10.intermediate.dense.bias: torch.Size([3072])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.10.output.dense\n",
      "  ✓ encoder.layer.10.output.dense.weight: torch.Size([768, 3072])\n",
      "  ✓ encoder.layer.10.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.10.output.LayerNorm\n",
      "  ✓ encoder.layer.10.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.10.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.11.attention.self.query\n",
      "  ✓ encoder.layer.11.attention.self.query.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.11.attention.self.query.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.11.attention.self.key\n",
      "  ✓ encoder.layer.11.attention.self.key.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.11.attention.self.key.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.11.attention.self.value\n",
      "  ✓ encoder.layer.11.attention.self.value.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.11.attention.self.value.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.11.attention.output.dense\n",
      "  ✓ encoder.layer.11.attention.output.dense.weight: torch.Size([768, 768])\n",
      "  ✓ encoder.layer.11.attention.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.11.attention.output.LayerNorm\n",
      "  ✓ encoder.layer.11.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.11.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.11.intermediate.dense\n",
      "  ✓ encoder.layer.11.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "  ✓ encoder.layer.11.intermediate.dense.bias: torch.Size([3072])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.11.output.dense\n",
      "  ✓ encoder.layer.11.output.dense.weight: torch.Size([768, 3072])\n",
      "  ✓ encoder.layer.11.output.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: encoder.layer.11.output.LayerNorm\n",
      "  ✓ encoder.layer.11.output.LayerNorm.weight: torch.Size([768])\n",
      "  ✓ encoder.layer.11.output.LayerNorm.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n",
      "\n",
      "Processing layer: pooler.dense\n",
      "  ✓ pooler.dense.weight: torch.Size([768, 768])\n",
      "  ✓ pooler.dense.bias: torch.Size([768])\n",
      "  Layer summary: 2/2 parameters loaded\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "weight_path = \"/path_to_your_downloaded/pytorch_model.bin\"\n",
    "weight_path = \"/home/markus/data/bert_base_uncased/pytorch_model.bin\"\n",
    "\n",
    "weight_loader = WeightLoader(strict=True, verbose=True)\n",
    "state_dict = weight_loader.load_pytorch_bin(weight_path)\n",
    "new_state_dict = weight_loader._apply_prefix_mapping(state_dict, prefix_mapping={\"bert.\":\"\"})\n",
    "new_state_dict = weight_loader._map_layernorm_keys(new_state_dict)\n",
    "\n",
    "weight_loader.layer_wise_loading(bk_bert_model, new_state_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559facc1",
   "metadata": {},
   "source": [
    "### 6. Tokenizing with BERTKit tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d74117fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokendata = bk_tokenizer.tokenize(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9748ebe0",
   "metadata": {},
   "source": [
    "### 7. Text inference and embedding generation with Kit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18ede8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position ids: tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23]])\n",
      "shape toty embeddings: torch.Size([3, 24, 768])\n",
      "shape embeddings: torch.Size([3, 24, 768])\n",
      "shape word embeddings: torch.Size([3, 24, 768])\n",
      "shape position embeddings: torch.Size([1, 24, 768])\n",
      "embeddings shape: torch.Size([3, 24, 768])\n",
      "type of encoder outputs:  <class 'tuple'> 3\n",
      "dict_keys(['last_hidden_state', 'pooler_output', 'hidden_states', 'attentions'])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = bk_bert_model(**tokendata)   # here outputs is a pure dataclass\n",
    "print(outputs.__dict__.keys())  # not directly addressable as a dict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c2d72a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last hidden states shape: torch.Size([3, 24, 768])\n",
      "Pooler output shape: torch.Size([3, 768])\n",
      "# of hidden states tensors: 13\n",
      "# of attention tensors: 12\n"
     ]
    }
   ],
   "source": [
    "print(f\"Last hidden states shape: {outputs.last_hidden_state.shape}\")\n",
    "print(f\"Pooler output shape: {outputs.pooler_output.shape}\")\n",
    "print(f\"# of hidden states tensors: {len(outputs.hidden_states)}\")\n",
    "print(f\"# of attention tensors: {len(outputs.attentions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d539c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2303, -0.4416, -0.2611,  ..., -0.7063, -0.0347,  0.6870],\n",
       "        [-0.1313, -0.1393, -0.3229,  ..., -0.0999,  0.0072,  0.2108],\n",
       "        [-0.2609, -0.1264, -0.3669,  ..., -0.1132,  0.1674,  0.1579]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings are the representation of the CLS token in each batch (sequence)\n",
    "# This representation is trained during pretraining:\n",
    "# expected output:\n",
    "#tensor([[-0.2303, -0.4416, -0.2611,  ..., -0.7063, -0.0347,  0.6870],\n",
    "#        [-0.1313, -0.1393, -0.3229,  ..., -0.0999,  0.0072,  0.2108],\n",
    "#        [-0.2609, -0.1264, -0.3669,  ..., -0.1132,  0.1674,  0.1579]])\n",
    "\n",
    "outputs.last_hidden_state[:, 0, :]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4e9e78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8546, -0.5405, -0.8914,  ..., -0.8892, -0.6927,  0.8527],\n",
       "        [-0.6780, -0.1119,  0.6431,  ...,  0.3919, -0.5026,  0.6840],\n",
       "        [-0.8838, -0.5582, -0.9850,  ..., -0.9025, -0.7519,  0.8843]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.pooler_output\n",
    "# expected output:\n",
    "#tensor([[-0.8546, -0.5405, -0.8914,  ..., -0.8892, -0.6927,  0.8527],\n",
    "#        [-0.6780, -0.1119,  0.6431,  ...,  0.3919, -0.5026,  0.6840],\n",
    "#        [-0.8838, -0.5582, -0.9850,  ..., -0.9025, -0.7519,  0.8843]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceac7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1db54f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
