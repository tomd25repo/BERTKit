{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b977ff",
   "metadata": {},
   "source": [
    "### The original HF BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f5b8dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7592,  1010,  1045,  2572, 14324,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101, 21641,  7861,  8270,  4667,  2015,  2024,  2019,  2590,  4145,\n",
       "          1998,  3710,  2004,  1037,  3978,  2005,  2592, 26384,   102],\n",
       "        [  101,  2070, 10962,  2024, 26191,  1010,  4589,  2015,  1010,  7222,\n",
       "         23804,  2015,  1010, 13137, 20968,   102,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if necessary:\n",
    "# !pip install transfomers\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "hf_tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "textm = [\"Hello, I am BERT\",\n",
    "         \"Semantic embeddings are an important concept and serve as a basis for information retrieval\",\n",
    "         \"Some fruits are bananas, oranges, pineapples, strawberries\"]\n",
    "hf_tokenizer(textm, return_tensors='pt', padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1c1671",
   "metadata": {},
   "source": [
    "### The decoupled BERTKit tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b40d8cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertkit_tokenizer.bertkit_tokenizer import BertKitTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e59ab51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> len vocab: 30522\n"
     ]
    }
   ],
   "source": [
    "bk_tokenizer = BertKitTokenizer(\"./bertkit_tokenizer/vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdaf54f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7592,  1010,  1045,  2572, 14324,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101, 21641,  7861,  8270,  4667,  2015,  2024,  2019,  2590,  4145,\n",
       "           1998,  3710,  2004,  1037,  3978,  2005,  2592, 26384,   102],\n",
       "         [  101,  2070, 10962,  2024, 26191,  1010,  4589,  2015,  1010,  7222,\n",
       "          23804,  2015,  1010, 13137, 20968,   102,     0,     0,     0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bk_tokenizer.tokenize(textm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c12039c",
   "metadata": {},
   "source": [
    "### Systematic tokenizer comparison for identical returns, done on a Gutenberg text containing also French with accents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdb3945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./bertkit_tokenizer/gutenberg_text.txt\", \"r\") as f:\n",
    "    text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "661a74f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def dicts_equal(d1, d2):\n",
    "    if d1.keys() != d2.keys():\n",
    "        return False\n",
    "    return all(torch.equal(d1[k], d2[k]) for k in d1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94bcf633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 sentence samples tested for identical tokenization\n",
      "10000 sentence samples tested for identical tokenization\n",
      "15000 sentence samples tested for identical tokenization\n",
      "20000 sentence samples tested for identical tokenization\n",
      "all 20058 samples ok.\n"
     ]
    }
   ],
   "source": [
    "for n, sentence in enumerate(text):\n",
    "    if n%5000 == 0 and n>0:    \n",
    "        print(f\"{n} sentence samples tested for identical tokenization\")\n",
    "    output_hf = hf_tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "    output_bk = bk_tokenizer.tokenize(sentence)\n",
    "    assert dicts_equal(output_hf,output_bk)\n",
    "print(f\"all {n} samples ok.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84182d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
